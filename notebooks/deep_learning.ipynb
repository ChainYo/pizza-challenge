{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Lightning and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, callbacks, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = \"../data/04_feature/preprocessed_data.json\"\n",
    "with open(datafile_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_was_edited', 'request_text_edit_aware', 'request_title',\n",
       "       'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_at_retrieval',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request', 'requester_received_pizza',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc',\n",
       "       'cleaned_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accepted = df[df[\"requester_received_pizza\"] == 1].shape[0]\n",
    "refused = df[df[\"requester_received_pizza\"] == 0].shape[0]\n",
    "total = accepted + refused\n",
    "\n",
    "plt.pie([accepted, refused], labels=[\"accepted\", \"refused\"], autopct=\"%1.1f%%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is really unbalanced, but we can still train a model on it and adapt weights when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in df[\"cleaned_text\"]]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2424, 2424\n",
      "Validation set size: 808, 808\n",
      "Test set size: 808, 808\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"requester_received_pizza\"], test_size=0.2, random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=3, stratify=y_train)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_val = y_val.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, {len(y_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}, {len(y_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}, {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `DistilBERT` as our base model.\n",
    "\n",
    "Why not `BERT`?\n",
    "\n",
    "The distillation of the original model will give us a faster model with good performances on downstream tasks like classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Simple classifier class to say if the request deserve a pizza or not\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_seq_len: int = 400, \n",
    "        batch_size: int = 256, \n",
    "        learning_rate: float = 1e-3,\n",
    "        n_classes: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the parameters given and add new layers for our downstream task\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            Maximum length of the sequence used to pad the input, default 400\n",
    "        batch_size: int\n",
    "            Batch size for training, default is 256\n",
    "        learning_rate: float\n",
    "            Learning rate for the optimizer of the model, default 1e-3\n",
    "        n_classes: int\n",
    "            Number of classes in the dataset, default is None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        if n_classes is not None:\n",
    "            self.n_classes = n_classes\n",
    "        else:\n",
    "            raise ValueError(\"n_classes must be specified.\")\n",
    "\n",
    "        # Training metrics\n",
    "        self.loss = nn.CrossEntropyLoss(weight=torch.tensor([0.68, 2.0]))\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "        self.test_accuracy = torchmetrics.Accuracy()\n",
    "        # self.confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=self.n_classes)\n",
    "        # self.precision = torchmetrics.Precision(num_classes=self.n_classes, average=None)\n",
    "        # self.recall = torchmetrics.Recall(num_classes=self.n_classes)\n",
    "        # self.f1_score = torchmetrics.F1Score(num_classes=self.n_classes)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\", num_labels=self.n_classes)\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False # Freeze all the weights and prevent the existing layers from training\n",
    "\n",
    "        self.classification_layers = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, self.n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, encode_id: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        encode_id: torch.Tensor\n",
    "            Tensor of shape (batch_size, max_seq_len) containing the encoded ids of the input sequence\n",
    "        mask: torch.Tensor\n",
    "            Tensor of shape (batch_size, max_seq_len) containing the mask of the input sequence\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor of shape (batch_size, 2) containing the logits of the model\n",
    "        \"\"\"\n",
    "        output = self.model(encode_id, attention_mask=mask)\n",
    "        logits = self.classification_layers(output[\"last_hidden_state\"])\n",
    "        return logits[:, -1]\n",
    "\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the data for the model and prepare it for training, validation and testing.\n",
    "        \"\"\"\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Tokenize and encode the input sequences for training, validation and testing\n",
    "        tokens_train = self.tokenizer.batch_encode_plus(\n",
    "            X_train, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_val = self.tokenizer.batch_encode_plus(\n",
    "            X_val, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_test = self.tokenizer.batch_encode_plus(\n",
    "            X_test, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        self.train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
    "        self.val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
    "        self.test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
    "\n",
    "        self.train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
    "        self.val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
    "        self.test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
    "\n",
    "        self.train_labels = torch.tensor(y_train)\n",
    "        self.val_labels = torch.tensor(y_val)\n",
    "        self.test_labels = torch.tensor(y_test)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the training set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the training set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.train_seq, self.train_mask, self.train_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the validation set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the validation set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.val_seq, self.val_mask, self.val_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the testing set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the testing set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.test_seq, self.test_mask, self.test_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Training step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the training set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.train_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"train_accuracy\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"loss\": loss, \"train_accuracy\": self.train_accuracy}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Validation step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the validation set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"val_accuracy\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": self.val_accuracy}\n",
    "\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Test step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the test set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.test_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"test_accuracy\", self.test_accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": self.test_accuracy}  \n",
    "\n",
    "\n",
    "    def train_epoch_end(self, outs) -> None:\n",
    "        \"\"\"\n",
    "        End of the training epoch. Reset the training accuracy.\n",
    "        \"\"\"\n",
    "        self.train_accuracy.reset()\n",
    "\n",
    "    \n",
    "    def val_epoch_end(self, outs) -> None:\n",
    "        \"\"\"\n",
    "        End of the validation epoch. Compute the validation accurac and reset the validation metrics.\n",
    "        \"\"\"\n",
    "        self.val_accuracy.reset()\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim.AdamW:\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.AdamW\n",
    "            Optimizer for the model\n",
    "        \"\"\"\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4402/795439870.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
      "/tmp/ipykernel_4402/795439870.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
      "/tmp/ipykernel_4402/795439870.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
      "/tmp/ipykernel_4402/795439870.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
      "/tmp/ipykernel_4402/795439870.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
      "/tmp/ipykernel_4402/795439870.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss | 0     \n",
      "1 | train_accuracy        | Accuracy         | 0     \n",
      "2 | val_accuracy          | Accuracy         | 0     \n",
      "3 | test_accuracy         | Accuracy         | 0     \n",
      "4 | model                 | BertModel        | 109 M \n",
      "5 | classification_layers | Sequential       | 394 K \n",
      "-----------------------------------------------------------\n",
      "394 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "439.508   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  49%|████▉     | 50/102 [00:13<00:14,  3.67it/s, loss=0.587]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchainyo-mleng\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chainyo/code/pizza-challenge/notebooks/wandb/run-20220317_102508-1u3nps7z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chainyo-mleng/challenge/runs/1u3nps7z\" target=\"_blank\">ancient-wood-8</a></strong> to <a href=\"https://wandb.ai/chainyo-mleng/challenge\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_everything(42, workers=True)\n",
    "logger = WandbLogger(project=\"challenge\", entity=\"chainyo-mleng\")\n",
    "gpu_value = 1 if torch.cuda.is_available() else 0 # Check if GPU is available\n",
    "\n",
    "model = RequestClassifier(batch_size=32, n_classes=2)\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../data/06_models/checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5, \n",
    "    progress_bar_refresh_rate=10, \n",
    "    gpus=1, \n",
    "    logger=logger, \n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    deterministic=False,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adb55e0d7d06b22fe6d30f2cb3e94f7617271379578af157e6db74b02ef92f76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
