{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Lightning and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, callbacks, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = \"../data/04_feature/preprocessed_data.json\"\n",
    "with open(datafile_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_was_edited', 'request_text_edit_aware', 'request_title',\n",
       "       'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_at_retrieval',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request', 'requester_received_pizza',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc',\n",
       "       'cleaned_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2424, 2424\n",
      "Validation set size: 808, 808\n",
      "Test set size: 808, 808\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"requester_received_pizza\"], test_size=0.2, random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=3, stratify=y_train)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_val = y_val.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, {len(y_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}, {len(y_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}, {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `DistilBERT` as our base model.\n",
    "\n",
    "Why not `BERT`?\n",
    "\n",
    "The distillation of the original model will give us a faster model with good performances on downstream tasks like classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Simple classifier class to say if the request deserve a pizza or not\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len: int = 350, batch_size: int = 256, learning_rate: float = 1e-3) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the parameters given and add new layers for our downstream task\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            Maximum length of the sequence used to pad the input, default 350\n",
    "        batch_size: int\n",
    "            Batch size for training, default is 256\n",
    "        learning_rate: float\n",
    "            Learning rate for the optimizer of the model, default 1e-3\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.val_accuracy = torchmetrics.Accuracy(num_classes=2)\n",
    "        self.test_accuracy = torchmetrics.Accuracy(num_classes=2)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False # Freeze all the weights and prevent the existing layers from training\n",
    "\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, encode_id: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        encode_id: torch.Tensor\n",
    "            Tensor of shape (batch_size, max_seq_len) containing the encoded ids of the input sequence\n",
    "        mask: torch.Tensor\n",
    "            Tensor of shape (batch_size, max_seq_len) containing the mask of the input sequence\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor of shape (batch_size, 2) containing the logits of the model\n",
    "        \"\"\"\n",
    "        outputs = self.model(encode_id, attention_mask=mask, return_dict=False) # Get the embeddings from the model\n",
    "        logits = self.new_layers(outputs[0]) # Pass the embeddings through the new layers\n",
    "        return logits\n",
    "\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the data for the model and prepare it for training, validation and testing.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        # Tokenize and encode the input sequences for training, validation and testing\n",
    "        tokens_train = self.tokenizer.batch_encode_plus(\n",
    "            X_train, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_val = self.tokenizer.batch_encode_plus(\n",
    "            X_val, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_test = self.tokenizer.batch_encode_plus(\n",
    "            X_test, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        self.train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
    "        self.val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
    "        self.test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
    "\n",
    "        self.train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
    "        self.val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
    "        self.test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
    "\n",
    "        self.train_labels = torch.tensor(y_train)\n",
    "        self.val_labels = torch.tensor(y_val)\n",
    "        self.test_labels = torch.tensor(y_test)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the training set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the training set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.train_seq, self.train_mask, self.train_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the validation set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the validation set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.val_seq, self.val_mask, self.val_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the testing set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the testing set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.test_seq, self.test_mask, self.test_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Training step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the training set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self.forward(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_accuracy = accuracy_score(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"train_accuracy\", train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Validation step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the validation set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self.forward(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"val_accuracy\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": self.val_accuracy}\n",
    "\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Test step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the test set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self.forward(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.test_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"test_accuracy\", self.test_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": self.test_accuracy}  \n",
    "\n",
    "    \n",
    "    def val_epoch_end(self, outs) -> None:\n",
    "        \"\"\"\n",
    "        End of the validation epoch. Compute the validation accuracy.\n",
    "        \"\"\"\n",
    "        total_val_accuracy = self.val_accuracy.compute()\n",
    "        self.log(\"total_val_accuracy\", total_val_accuracy, on_step=False, on_epoch=True)\n",
    "        print(f\"Total validation accuracy: {total_val_accuracy}\")\n",
    "\n",
    "    \n",
    "    def test_epoch_end(self, outs) -> None:\n",
    "        \"\"\"\n",
    "        End of the test epoch. Compute the test accuracy.\n",
    "        \"\"\"\n",
    "        total_test_accuracy = self.test_accuracy.compute()\n",
    "        self.log(\"total_test_accuracy\", total_test_accuracy, on_step=False, on_epoch=True)\n",
    "        print(f\"Total test accuracy: {total_test_accuracy}\")\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim.AdamW:\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.AdamW\n",
    "            Optimizer for the model\n",
    "        \"\"\"\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/tmp/ipykernel_13615/655279133.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
      "/tmp/ipykernel_13615/655279133.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
      "/tmp/ipykernel_13615/655279133.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
      "/tmp/ipykernel_13615/655279133.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
      "/tmp/ipykernel_13615/655279133.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
      "/tmp/ipykernel_13615/655279133.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0     \n",
      "1 | val_accuracy  | Accuracy         | 0     \n",
      "2 | test_accuracy | Accuracy         | 0     \n",
      "3 | model         | DistilBertModel  | 66.4 M\n",
      "4 | new_layers    | Sequential       | 394 K \n",
      "---------------------------------------------------\n",
      "394 K     Trainable params\n",
      "66.4 M    Non-trainable params\n",
      "66.8 M    Total params\n",
      "267.028   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]tensor([[347],\n",
      "        [ 57],\n",
      "        [ 76],\n",
      "        [105],\n",
      "        [ 50],\n",
      "        [ 82],\n",
      "        [279],\n",
      "        [125],\n",
      "        [181],\n",
      "        [111],\n",
      "        [ 22],\n",
      "        [196],\n",
      "        [ 18],\n",
      "        [  9],\n",
      "        [ 57],\n",
      "        [ 55],\n",
      "        [102],\n",
      "        [ 20],\n",
      "        [ 49],\n",
      "        [ 26],\n",
      "        [137],\n",
      "        [ 14],\n",
      "        [186],\n",
      "        [141],\n",
      "        [ 57],\n",
      "        [346],\n",
      "        [ 17],\n",
      "        [ 28],\n",
      "        [ 60],\n",
      "        [ 32],\n",
      "        [ 36],\n",
      "        [ 19]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If `preds` have one dimension more than `target`, `preds` should be a float tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=13'>14</a>\u001b[0m early_stopping_callback \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mEarlyStopping(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=14'>15</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=15'>16</a>\u001b[0m     patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=16'>17</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=17'>18</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=21'>22</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=22'>23</a>\u001b[0m     progress_bar_refresh_rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=26'>27</a>\u001b[0m     deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=27'>28</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39mtest(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=734'>735</a>\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=735'>736</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=736'>737</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=737'>738</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=738'>739</a>\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=739'>740</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=740'>741</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=741'>742</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=674'>675</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=675'>676</a>\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=676'>677</a>\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=681'>682</a>\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=682'>683</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=683'>684</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=684'>685</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=685'>686</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=686'>687</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=774'>775</a>\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=775'>776</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=776'>777</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=778'>779</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=779'>780</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1195'>1196</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1197'>1198</a>\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1198'>1199</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1200'>1201</a>\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1201'>1202</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1276'>1277</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1277'>1278</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1278'>1279</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=200'>201</a>\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=201'>202</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1286'>1287</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1287'>1288</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1288'>1289</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1311\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1307'>1308</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1308'>1309</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1310'>1311</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1312'>1313</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1313'>1314</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1375\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1372'>1373</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1374'>1375</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1376'>1377</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1378'>1379</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:110\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=104'>105</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=105'>106</a>\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=106'>107</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=107'>108</a>\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=109'>110</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=111'>112</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=112'>113</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:122\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=119'>120</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=120'>121</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=121'>122</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=122'>123</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=124'>125</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:217\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=214'>215</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=215'>216</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=216'>217</a>\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mvalidation_step(step_kwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=218'>219</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:239\u001b[0m, in \u001b[0;36mAccelerator.validation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=233'>234</a>\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=234'>235</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=235'>236</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=236'>237</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=237'>238</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=238'>239</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:219\u001b[0m, in \u001b[0;36mTrainingTypePlugin.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=217'>218</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=218'>219</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb Cell 8'\u001b[0m in \u001b[0;36mRequestClassifier.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=197'>198</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=198'>199</a>\u001b[0m \u001b[39mprint\u001b[39m(preds)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=199'>200</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_accuracy(preds, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=200'>201</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(outputs, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=201'>202</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_accuracy, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, on_step\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py:205\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=198'>199</a>\u001b[0m     \u001b[39mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=199'>200</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe Metric shouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be synced when performing ``update``. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=200'>201</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=201'>202</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=203'>204</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=206'>207</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_step:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=207'>208</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_on_step\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py:263\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=260'>261</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=261'>262</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_called \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/metric.py?line=262'>263</a>\u001b[0m \u001b[39mreturn\u001b[39;00m update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py:228\u001b[0m, in \u001b[0;36mAccuracy.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=218'>219</a>\u001b[0m \u001b[39m\"\"\"Update state with predictions and targets. See\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=219'>220</a>\u001b[0m \u001b[39m:ref:`references/modules:input types` for more information on input\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=220'>221</a>\u001b[0m \u001b[39mtypes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=224'>225</a>\u001b[0m \u001b[39m    target: Ground truth labels\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=225'>226</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=226'>227</a>\u001b[0m \u001b[39m\"\"\" returns the mode of the data (binary, multi label, multi class, multi-dim multi class) \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=227'>228</a>\u001b[0m mode \u001b[39m=\u001b[39m _mode(preds, target, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthreshold, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtop_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulticlass)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=229'>230</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/classification/accuracy.py?line=230'>231</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py:58\u001b[0m, in \u001b[0;36m_mode\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass)\u001b[0m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mode\u001b[39m(\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=29'>30</a>\u001b[0m     preds: Tensor,\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=30'>31</a>\u001b[0m     target: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=34'>35</a>\u001b[0m     multiclass: Optional[\u001b[39mbool\u001b[39m],\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=35'>36</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataType:\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=36'>37</a>\u001b[0m     \u001b[39m\"\"\"Finds the mode of the input tensors.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=37'>38</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=38'>39</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=54'>55</a>\u001b[0m \u001b[39m        <DataType.MULTICLASS: 'multi-class'>\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=55'>56</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=57'>58</a>\u001b[0m     mode \u001b[39m=\u001b[39m _check_classification_inputs(\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=58'>59</a>\u001b[0m         preds, target, threshold\u001b[39m=\u001b[39;49mthreshold, top_k\u001b[39m=\u001b[39;49mtop_k, num_classes\u001b[39m=\u001b[39;49mnum_classes, multiclass\u001b[39m=\u001b[39;49mmulticlass\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=59'>60</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mode\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:254\u001b[0m, in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=250'>251</a>\u001b[0m _basic_input_validation(preds, target, threshold, multiclass)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=252'>253</a>\u001b[0m \u001b[39m# Check that shape/types fall into one of the cases\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=253'>254</a>\u001b[0m case, implied_classes \u001b[39m=\u001b[39m _check_shape_and_type_consistency(preds, target)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=255'>256</a>\u001b[0m \u001b[39m# Check consistency with the `C` dimension in case of multi-class data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=256'>257</a>\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m target\u001b[39m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:87\u001b[0m, in \u001b[0;36m_check_shape_and_type_consistency\u001b[0;34m(preds, target)\u001b[0m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=84'>85</a>\u001b[0m \u001b[39melif\u001b[39;00m preds\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m target\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=85'>86</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m preds_float:\n\u001b[0;32m---> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=86'>87</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf `preds` have one dimension more than `target`, `preds` should be a float tensor.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=87'>88</a>\u001b[0m     \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:] \u001b[39m!=\u001b[39m target\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=88'>89</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=89'>90</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf `preds` have one dimension more than `target`, the shape of `preds` should be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=90'>91</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m (N, C, ...), and the shape of `target` should be (N, ...).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torchmetrics/utilities/checks.py?line=91'>92</a>\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: If `preds` have one dimension more than `target`, `preds` should be a float tensor."
     ]
    }
   ],
   "source": [
    "seed_everything(42, workers=True)\n",
    "logger = WandbLogger(project=\"challenge\")\n",
    "gpu_value = 1 if torch.cuda.is_available() else 0 # Check if GPU is available\n",
    "\n",
    "model = RequestClassifier(batch_size=32)\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../data/checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10, \n",
    "    progress_bar_refresh_rate=10, \n",
    "    gpus=1, \n",
    "    logger=logger, \n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adb55e0d7d06b22fe6d30f2cb3e94f7617271379578af157e6db74b02ef92f76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
