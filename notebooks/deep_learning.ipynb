{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Lightning and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, callbacks, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = \"../data/04_feature/preprocessed_data.json\"\n",
    "with open(datafile_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_was_edited', 'request_text_edit_aware', 'request_title',\n",
       "       'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_at_retrieval',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request', 'requester_received_pizza',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc',\n",
       "       'cleaned_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in df[\"cleaned_text\"]]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2424, 2424\n",
      "Validation set size: 808, 808\n",
      "Test set size: 808, 808\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"requester_received_pizza\"], test_size=0.2, random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=3, stratify=y_train)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_val = y_val.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, {len(y_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}, {len(y_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}, {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `DistilBERT` as our base model.\n",
    "\n",
    "Why not `BERT`?\n",
    "\n",
    "The distillation of the original model will give us a faster model with good performances on downstream tasks like classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Simple classifier class to say if the request deserve a pizza or not\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_seq_len: int = 400, \n",
    "        batch_size: int = 256, \n",
    "        learning_rate: float = 1e-3,\n",
    "        n_classes: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the parameters given and add new layers for our downstream task\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            Maximum length of the sequence used to pad the input, default 400\n",
    "        batch_size: int\n",
    "            Batch size for training, default is 256\n",
    "        learning_rate: float\n",
    "            Learning rate for the optimizer of the model, default 1e-3\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        if n_classes is not None:\n",
    "            self.n_classes = n_classes\n",
    "        else:\n",
    "            raise ValueError(\"n_classes must be specified.\")\n",
    "\n",
    "        # Training metrics\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "        self.test_accuracy = torchmetrics.Accuracy()\n",
    "        self.confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=self.n_classes, normalize=\"all\")\n",
    "        # self.precision = torchmetrics.Precision(num_classes=self.n_classes, average=\"macro\")\n",
    "        self.recall = torchmetrics.Recall(num_classes=self.n_classes, average=\"macro\")\n",
    "        self.f1_score = torchmetrics.F1Score(num_classes=self.n_classes, average=\"macro\")\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\", num_labels=self.n_classes)\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False # Freeze all the weights and prevent the existing layers from training\n",
    "\n",
    "        self.classification_layers = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, self.n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, encode_id: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        encode_id: torch.Tensor\n",
    "            Tensor of shape (batch_size, max_seq_len) containing the encoded ids of the input sequence\n",
    "        mask: torch.Tensor\n",
    "            Tensor of shape (batch_size, max_seq_len) containing the mask of the input sequence\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor of shape (batch_size, 2) containing the logits of the model\n",
    "        \"\"\"\n",
    "        output = self.model(encode_id, attention_mask=mask)\n",
    "        logits = self.classification_layers(output[\"last_hidden_state\"])\n",
    "        return logits[:, -1]\n",
    "\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the data for the model and prepare it for training, validation and testing.\n",
    "        \"\"\"\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Tokenize and encode the input sequences for training, validation and testing\n",
    "        tokens_train = self.tokenizer.batch_encode_plus(\n",
    "            X_train, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_val = self.tokenizer.batch_encode_plus(\n",
    "            X_val, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_test = self.tokenizer.batch_encode_plus(\n",
    "            X_test, \n",
    "            max_length=self.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        self.train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
    "        self.val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
    "        self.test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
    "\n",
    "        self.train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
    "        self.val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
    "        self.test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
    "\n",
    "        self.train_labels = torch.tensor(y_train)\n",
    "        self.val_labels = torch.tensor(y_val)\n",
    "        self.test_labels = torch.tensor(y_test)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the training set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the training set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.train_seq, self.train_mask, self.train_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the validation set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the validation set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.val_seq, self.val_mask, self.val_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a dataloader for the testing set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            Dataloader for the testing set with the batch size specified in the constructor.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.test_seq, self.test_mask, self.test_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Training step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the training set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.train_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"train_accuracy\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"loss\": loss, \"train_accuracy\": self.train_accuracy}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Validation step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the validation set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"val_accuracy\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": self.val_accuracy}\n",
    "\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Test step of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Tuple containing the encoded ids, mask and labels of the batch\n",
    "        batch_idx: int\n",
    "            Index of the batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict containing the loss and the accuracy of the model for the test set\n",
    "        \"\"\"\n",
    "        encode_id, mask, labels = batch\n",
    "        outputs = self(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.test_accuracy(preds, labels)\n",
    "        self.confusion_matrix(preds, labels)\n",
    "        # self.precision(preds, labels)\n",
    "        self.recall(preds, labels)\n",
    "        self.f1_score(preds, labels)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"test_accuracy\", self.test_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": self.test_accuracy}  \n",
    "\n",
    "    \n",
    "    def val_epoch_end(self, outs) -> None:\n",
    "        \"\"\"\n",
    "        End of the validation epoch. Compute the validation accuracy.\n",
    "        \"\"\"\n",
    "        total_val_accuracy = self.val_accuracy.compute()\n",
    "        self.log(\"total_val_accuracy\", total_val_accuracy, on_step=False, on_epoch=True)\n",
    "        print(f\"Total validation accuracy: {total_val_accuracy}\")\n",
    "\n",
    "    \n",
    "    def test_epoch_end(self, outs) -> None:\n",
    "        \"\"\"\n",
    "        End of the test epoch. Compute the test accuracy.\n",
    "        \"\"\"\n",
    "        total_test_accuracy = self.test_accuracy.compute()\n",
    "        self.log(\"total_test_accuracy\", total_test_accuracy, on_step=False, on_epoch=True)\n",
    "        self.log(\"confusion_matrix\", self.confusion_matrix, on_step=False, on_epoch=True)\n",
    "        # self.log(\"precision\", self.precision, on_step=False, on_epoch=True)\n",
    "        self.log(\"recall\", self.recall, on_step=False, on_epoch=True)\n",
    "        self.log(\"f1_score\", self.f1_score, on_step=False, on_epoch=True)\n",
    "        print(f\"Total test accuracy: {total_test_accuracy}\")\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim.AdamW:\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.AdamW\n",
    "            Optimizer for the model\n",
    "        \"\"\"\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4913/2094096549.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
      "/tmp/ipykernel_4913/2094096549.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
      "/tmp/ipykernel_4913/2094096549.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
      "/tmp/ipykernel_4913/2094096549.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
      "/tmp/ipykernel_4913/2094096549.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
      "/tmp/ipykernel_4913/2094096549.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss | 0     \n",
      "1 | train_accuracy        | Accuracy         | 0     \n",
      "2 | val_accuracy          | Accuracy         | 0     \n",
      "3 | test_accuracy         | Accuracy         | 0     \n",
      "4 | confusion_matrix      | ConfusionMatrix  | 0     \n",
      "5 | recall                | Recall           | 0     \n",
      "6 | f1                    | F1Score          | 0     \n",
      "7 | model                 | BertModel        | 109 M \n",
      "8 | classification_layers | Sequential       | 394 K \n",
      "-----------------------------------------------------------\n",
      "394 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "439.508   Total estimated model params size (MB)\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/chainyo/code/ca-chlng/data/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  98%|█████████▊| 100/102 [00:28<00:00,  3.51it/s, loss=0.604, v_num=t0sk, val_accuracy=0.722, val_loss=0.583]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 102/102 [00:28<00:00,  3.58it/s, loss=0.604, v_num=t0sk, val_accuracy=0.722, val_loss=0.583, train_accuracy=0.720, train_loss=0.599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 75: val_loss reached 0.58341 (best 0.58341), saving model to \"/home/chainyo/code/ca-chlng/data/checkpoints/epoch=0-step=75-v2.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 100/102 [00:27<00:00,  3.63it/s, loss=0.596, v_num=t0sk, val_accuracy=0.752, val_loss=0.576, train_accuracy=0.720, train_loss=0.599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 102/102 [00:27<00:00,  3.70it/s, loss=0.596, v_num=t0sk, val_accuracy=0.752, val_loss=0.576, train_accuracy=0.734, train_loss=0.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 151: val_loss reached 0.57604 (best 0.57604), saving model to \"/home/chainyo/code/ca-chlng/data/checkpoints/epoch=1-step=151.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  98%|█████████▊| 100/102 [00:27<00:00,  3.69it/s, loss=0.574, v_num=t0sk, val_accuracy=0.749, val_loss=0.567, train_accuracy=0.734, train_loss=0.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 102/102 [00:27<00:00,  3.76it/s, loss=0.574, v_num=t0sk, val_accuracy=0.749, val_loss=0.567, train_accuracy=0.745, train_loss=0.551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 227: val_loss reached 0.56695 (best 0.56695), saving model to \"/home/chainyo/code/ca-chlng/data/checkpoints/epoch=2-step=227.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  98%|█████████▊| 100/102 [00:27<00:00,  3.67it/s, loss=0.538, v_num=t0sk, val_accuracy=0.750, val_loss=0.567, train_accuracy=0.745, train_loss=0.551]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 102/102 [00:27<00:00,  3.74it/s, loss=0.538, v_num=t0sk, val_accuracy=0.750, val_loss=0.567, train_accuracy=0.749, train_loss=0.537]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 303: val_loss reached 0.56685 (best 0.56685), saving model to \"/home/chainyo/code/ca-chlng/data/checkpoints/epoch=3-step=303-v2.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 102/102 [00:27<00:00,  3.70it/s, loss=0.537, v_num=t0sk, val_accuracy=0.750, val_loss=0.577, train_accuracy=0.754, train_loss=0.522]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 379: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 102/102 [00:27<00:00,  3.74it/s, loss=0.515, v_num=t0sk, val_accuracy=0.749, val_loss=0.577, train_accuracy=0.759, train_loss=0.509]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 455: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 102/102 [00:27<00:00,  3.75it/s, loss=0.513, v_num=t0sk, val_accuracy=0.745, val_loss=0.591, train_accuracy=0.764, train_loss=0.492]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 531: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 102/102 [00:27<00:00,  3.66it/s, loss=0.472, v_num=t0sk, val_accuracy=0.740, val_loss=0.601, train_accuracy=0.782, train_loss=0.479]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 607: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  98%|█████████▊| 100/102 [00:28<00:00,  3.50it/s, loss=0.453, v_num=t0sk, val_accuracy=0.751, val_loss=0.635, train_accuracy=0.782, train_loss=0.479]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.567. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 102/102 [00:28<00:00,  3.57it/s, loss=0.453, v_num=t0sk, val_accuracy=0.751, val_loss=0.635, train_accuracy=0.798, train_loss=0.445]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 683: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 102/102 [00:28<00:00,  3.57it/s, loss=0.453, v_num=t0sk, val_accuracy=0.751, val_loss=0.635, train_accuracy=0.798, train_loss=0.445]\n",
      "Epoch 0:  98%|█████████▊| 100/102 [2:20:03<02:48, 84.04s/it, loss=0.604, v_num=t0sk, val_accuracy=0.722]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RequestClassifier' object has no attribute 'f1_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=21'>22</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=22'>23</a>\u001b[0m     progress_bar_refresh_rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=26'>27</a>\u001b[0m     deterministic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=27'>28</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000008?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:911\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, test_dataloaders)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=905'>906</a>\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=906'>907</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.test(test_dataloaders)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=907'>908</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.test(dataloaders)` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=908'>909</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=909'>910</a>\u001b[0m     dataloaders \u001b[39m=\u001b[39m test_dataloaders\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=910'>911</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=674'>675</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=675'>676</a>\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=676'>677</a>\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=681'>682</a>\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=682'>683</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=683'>684</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=684'>685</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=685'>686</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=686'>687</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:954\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=948'>949</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtested_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=949'>950</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=950'>951</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=952'>953</a>\u001b[0m \u001b[39m# run test\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=953'>954</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtested_ckpt_path)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=955'>956</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=956'>957</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1195'>1196</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1197'>1198</a>\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1198'>1199</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1200'>1201</a>\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1201'>1202</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1275\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1272'>1273</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_dispatch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1273'>1274</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1274'>1275</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_evaluating(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1275'>1276</a>\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1276'>1277</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:206\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_evaluating\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=203'>204</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_evaluating\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=204'>205</a>\u001b[0m     \u001b[39m# double dispatch to initiate the test loop\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1286\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1282'>1283</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__setup_profiler()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1284'>1285</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1285'>1286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_evaluate()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1286'>1287</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1287'>1288</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1334\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1330'>1331</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1332'>1333</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage\u001b[39m}\u001b[39;00m\u001b[39m_evaluation\u001b[39m\u001b[39m\"\u001b[39m), torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1333'>1334</a>\u001b[0m     eval_loop_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1335'>1336</a>\u001b[0m \u001b[39m# remove the tensors from the eval results\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1336'>1337</a>\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m eval_loop_results:\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:110\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=104'>105</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=105'>106</a>\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=106'>107</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=107'>108</a>\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=109'>110</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=111'>112</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=112'>113</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:122\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=119'>120</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=120'>121</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=121'>122</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=122'>123</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=124'>125</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:213\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=210'>211</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=211'>212</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=212'>213</a>\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mtest_step(step_kwargs)\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=213'>214</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=214'>215</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:247\u001b[0m, in \u001b[0;36mAccelerator.test_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=241'>242</a>\u001b[0m \u001b[39m\"\"\"The actual test step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=242'>243</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=243'>244</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.test_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=244'>245</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=245'>246</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtest_step_context():\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py?line=246'>247</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:222\u001b[0m, in \u001b[0;36mTrainingTypePlugin.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=220'>221</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=221'>222</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb Cell 9'\u001b[0m in \u001b[0;36mRequestClassifier.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=243'>244</a>\u001b[0m \u001b[39m# self.precision(preds, labels)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=244'>245</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecall(preds, labels)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=245'>246</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf1_score(preds, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=246'>247</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(outputs, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/chainyo/code/ca-chlng/notebooks/deep_learning.ipynb#ch0000007?line=247'>248</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtest_accuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_accuracy, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, on_step\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1182'>1183</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1183'>1184</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1184'>1185</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///home/chainyo/miniconda3/envs/challenge/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1185'>1186</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RequestClassifier' object has no attribute 'f1_score'"
     ]
    }
   ],
   "source": [
    "seed_everything(42, workers=True)\n",
    "logger = WandbLogger(project=\"challenge\", entity=\"chainyo-mleng\")\n",
    "gpu_value = 1 if torch.cuda.is_available() else 0 # Check if GPU is available\n",
    "\n",
    "model = RequestClassifier(batch_size=32, n_classes=2)\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../data/checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50, \n",
    "    progress_bar_refresh_rate=10, \n",
    "    gpus=1, \n",
    "    logger=logger, \n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    deterministic=False,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adb55e0d7d06b22fe6d30f2cb3e94f7617271379578af157e6db74b02ef92f76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
